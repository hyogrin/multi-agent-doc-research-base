{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize AI Search\n",
    "\n",
    "This sample demonstrates how to create an index for Azure AI Search using semantic tagging and advanced filtering capabilities.\n",
    "\n",
    "## Environment Variables Configuration\n",
    "\n",
    "Update your `.env` file with the following variables for Document Intelligence and AI Search:\n",
    "\n",
    "```bash\n",
    "# Azure AI Search\n",
    "AZURE_AI_SEARCH_ENDPOINT=https://your-search-service.search.windows.net\n",
    "AZURE_AI_SEARCH_API_KEY=your-search-api-key\n",
    "AZURE_AI_SEARCH_INDEX_NAME=document_inquiry_index\n",
    "\n",
    "# Azure OpenAI\n",
    "AZURE_OPENAI_ENDPOINT=https://your-openai-service.openai.azure.com\n",
    "AZURE_OPENAI_API_KEY=your-openai-api-key\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME=text-embedding-3-small\n",
    "AZURE_OPENAI_EMBEDDING_DIMENSIONS=1536\n",
    "AZURE_OPENAI_API_VERSION=2024-12-01-preview\n",
    "\n",
    "# Azure Document Intelligence\n",
    "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://your-document-intelligence.cognitiveservices.azure.com\n",
    "AZURE_DOCUMENT_INTELLIGENCE_API_KEY=your-document-intelligence-api-key\n",
    "\n",
    "```\n",
    "\n",
    "> ✨ **_Note_** <br>\n",
    "> **Check Region support**: Please check the regional support for Azure AI Search before you get started - https://learn.microsoft.com/en-us/azure/search/search-region-support <br>\n",
    "> **Pricing tier for Semantic Search**: In order to use the Semantic Search feature, check your region availability and pricing tier. Make sure it is at least Standard S3. <br>\n",
    "> **Collection field limitation**: fuzzy search is not supported for collection fields.\n",
    "\n",
    "## Key Features:\n",
    "- **doc_upload_plugin**: Enables document upload and indexing into AI Search\n",
    "- **ai_search_plugin**: Search capabilities with semantic ranking\n",
    "\n",
    "\n",
    "Create an `.env` file based on the `sample.env` file. Copy the new `.env` file to the folder containing your notebook and update the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Environment Variables ====\n",
      "azure_search_endpoint=https://hyo-rag-ai-search.search.windows.net\n",
      "azure_openai_endpoint=https://hyo-ai-foundry-pjt1-resource.openai.azure.com/\n",
      "azure_openai_deployment_name=gpt-4.1\n",
      "azure_openai_embedding_deployment_name=text-embedding-3-small\n",
      "azure_openai_embedding_dimensions=1536\n",
      "index_name=doc_inquiry_index\n",
      "All environment variables are valid.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery,\n",
    "    VectorizableTextQuery,\n",
    "    VectorFilterMode,\n",
    ")\n",
    "from azure.search.documents.models import QueryType, QueryCaptionType, QueryAnswerType\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "# Validate environment variables\n",
    "def get_env_var(name, required=True):\n",
    "    value = os.getenv(name)\n",
    "    if required and not value:\n",
    "        raise ValueError(f\"Environment variable {name} is missing or empty.\")\n",
    "    return value\n",
    "\n",
    "\n",
    "try:\n",
    "    azure_ai_search_endpoint = os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\")\n",
    "    azure_ai_search_admin_key = get_env_var(\"AZURE_AI_SEARCH_API_KEY\", required=True)\n",
    "\n",
    "    azure_openai_endpoint = get_env_var(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    azure_openai_key = get_env_var(\"AZURE_OPENAI_API_KEY\")\n",
    "    azure_openai_deployment_name = get_env_var(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    azure_openai_embedding_deployment_name = (\n",
    "        get_env_var(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\", required=False)\n",
    "        or \"text-embedding-3-small\"\n",
    "    )\n",
    "    azure_openai_embedding_dimensions = int(\n",
    "        get_env_var(\"AZURE_OPENAI_EMBEDDING_DIMENSIONS\", required=False) or 1536\n",
    "    )\n",
    "    azure_openai_api_version = (\n",
    "        get_env_var(\"AZURE_OPENAI_API_VERSION\", required=False) or \"2024-12-01-preview\"\n",
    "    )\n",
    "\n",
    "    index_name = (\n",
    "        get_env_var(\"AZURE_AI_SEARCH_INDEX_NAME\", required=False)\n",
    "        or \"doc_inquiry_index\"\n",
    "    )\n",
    "\n",
    "    print(\"==== Environment Variables ====\")\n",
    "    print(f\"azure_search_endpoint={azure_ai_search_endpoint}\")\n",
    "    \n",
    "    print(f\"azure_openai_endpoint={azure_openai_endpoint}\")\n",
    "    \n",
    "    print(f\"azure_openai_deployment_name={azure_openai_deployment_name}\")\n",
    "    print(\n",
    "        f\"azure_openai_embedding_deployment_name={azure_openai_embedding_deployment_name}\"\n",
    "    )\n",
    "    print(f\"azure_openai_embedding_dimensions={azure_openai_embedding_dimensions}\")\n",
    "    print(f\"index_name={index_name}\")\n",
    "\n",
    "    # Validate credentials\n",
    "    if azure_ai_search_admin_key:\n",
    "        search_credential = AzureKeyCredential(azure_ai_search_admin_key)\n",
    "    else:\n",
    "        search_credential = DefaultAzureCredential()\n",
    "        # Check if DefaultAzureCredential works\n",
    "        try:\n",
    "            search_credential.get_token(\"https://management.azure.com/.default\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(\n",
    "                \"DefaultAzureCredential authentication failed. Ensure you are logged in using `az login`.\"\n",
    "            ) from e\n",
    "\n",
    "    print(\"All environment variables are valid.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"[ERROR] {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if azure_openai_endpoint is None:\n",
    "    raise ValueError(\"The Azure OpenAI endpoint is not set.\")\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vector index (Execute Once)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_client = SearchIndexClient(\n",
    "    endpoint=azure_ai_search_endpoint, credential=search_credential\n",
    ")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=azure_ai_search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=search_credential,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'doc_inquiry_index' deleted.\n",
      "Index 'doc_inquiry_index' created or updated successfully.\n"
     ]
    }
   ],
   "source": [
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "CREATE_INDEX = True\n",
    "\n",
    "if CREATE_INDEX:\n",
    "    \n",
    "    try:\n",
    "        index_client.get_index(index_name)\n",
    "        index_client.delete_index(index_name)\n",
    "        print(f\"Index '{index_name}' deleted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Index '{index_name}' does not exist, creating a new one.\")\n",
    "        print(f\"Error details: {e}\")\n",
    "    \n",
    "    # Enhanced document index fields for PDF/IR document processing\n",
    "    fields = [\n",
    "        # Primary identification fields\n",
    "        SimpleField(name=\"docId\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String, sortable=True, filterable=True),\n",
    "        SearchableField(name=\"file_name\", type=SearchFieldDataType.String, sortable=True, filterable=True),\n",
    "        SearchableField(name=\"keywords\", type=SearchFieldDataType.String, sortable=True, filterable=True),\n",
    "        \n",
    "        # Content fields\n",
    "        SearchableField(\n",
    "            name=\"content\", \n",
    "            type=SearchFieldDataType.String, \n",
    "            analyzer_name=\"en.lucene\"\n",
    "        ),\n",
    "        \n",
    "        SearchableField(name=\"summary\", type=SearchFieldDataType.String),\n",
    "        \n",
    "        \n",
    "        # Document structure fields\n",
    "        SimpleField(name=\"page_number\", type=SearchFieldDataType.Int32, filterable=True, sortable=True),\n",
    "        SimpleField(name=\"paragraph_number\", type=SearchFieldDataType.Int32, filterable=True, sortable=True),\n",
    "        SimpleField(name=\"chunk_number\", type=SearchFieldDataType.Int32, filterable=True, sortable=True),\n",
    "        \n",
    "        # Document categorization\n",
    "        SearchableField(\n",
    "            name=\"document_type\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            facetable=True,\n",
    "            filterable=True,\n",
    "            sortable=True,\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"company\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            facetable=True,\n",
    "            filterable=True,\n",
    "            sortable=True,\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"industry\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            facetable=True,\n",
    "            filterable=True,\n",
    "            sortable=True,\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"report_year\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            facetable=True,\n",
    "            filterable=True,\n",
    "            sortable=True,\n",
    "        ),\n",
    "        \n",
    "        # File metadata\n",
    "        SimpleField(name=\"file_hash\", type=SearchFieldDataType.String, filterable=True),\n",
    "        SimpleField(name=\"upload_date\", type=SearchFieldDataType.DateTimeOffset, filterable=True, sortable=True),\n",
    "        SearchableField(name=\"source\", type=SearchFieldDataType.String, filterable=True),\n",
    "        SearchableField(name=\"metadata\", type=SearchFieldDataType.String),\n",
    "        \n",
    "        \n",
    "        # Vector field for semantic search\n",
    "        SearchField(\n",
    "            name=\"content_vector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=azure_openai_embedding_dimensions,\n",
    "            vector_search_profile_name=\"content-vector-profile\",\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"summary_vector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=azure_openai_embedding_dimensions,\n",
    "            vector_search_profile_name=\"summary-vector-profile\",\n",
    "        ),\n",
    "        \n",
    "        # # Document tags for advanced filtering\n",
    "        # SearchField(\n",
    "        #     name=\"document_tags\",\n",
    "        #     type=SearchFieldDataType.Collection(SearchFieldDataType.String),\n",
    "        #     searchable=True,\n",
    "        #     filterable=True,\n",
    "        #     facetable=True,\n",
    "        # ),\n",
    "        # SearchField(\n",
    "        #     name=\"target_audience\",\n",
    "        #     type=SearchFieldDataType.Collection(SearchFieldDataType.String),\n",
    "        #     searchable=True,\n",
    "        #     filterable=True,\n",
    "        #     facetable=True,\n",
    "        # ),\n",
    "        # SearchField(\n",
    "        #     name=\"topics\",\n",
    "        #     type=SearchFieldDataType.Collection(SearchFieldDataType.String),\n",
    "        #     searchable=True,\n",
    "        #     filterable=True,\n",
    "        #     facetable=True,\n",
    "        # ),\n",
    "    ]\n",
    "\n",
    "    # Vector search configuration\n",
    "    vector_search = VectorSearch(\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"content-vector-profile\",\n",
    "                algorithm_configuration_name=\"content-hnsw-config\",\n",
    "                vectorizer_name=\"content-openai-vectorizer\",\n",
    "            ),\n",
    "            VectorSearchProfile(\n",
    "                name=\"summary-vector-profile\",\n",
    "                algorithm_configuration_name=\"summary-hnsw-config\",\n",
    "                vectorizer_name=\"summary-openai-vectorizer\",\n",
    "            ),\n",
    "        ],\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"content-hnsw-config\",\n",
    "                parameters={\n",
    "                    \"m\": 4,\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500,\n",
    "                    \"metric\": \"cosine\",\n",
    "                },\n",
    "            ),\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"summary-hnsw-config\",\n",
    "                parameters={\n",
    "                    \"m\": 4,\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500,\n",
    "                    \"metric\": \"cosine\",\n",
    "                },\n",
    "            ),\n",
    "        ],\n",
    "        vectorizers=[\n",
    "            AzureOpenAIVectorizer(\n",
    "                vectorizer_name=\"content-openai-vectorizer\",\n",
    "                parameters=AzureOpenAIVectorizerParameters(\n",
    "                    resource_url=azure_openai_endpoint,\n",
    "                    deployment_name=azure_openai_embedding_deployment_name,\n",
    "                    model_name=azure_openai_embedding_deployment_name,\n",
    "                    api_key=azure_openai_key,\n",
    "                ),\n",
    "            ),\n",
    "            AzureOpenAIVectorizer(\n",
    "                vectorizer_name=\"summary-openai-vectorizer\",\n",
    "                parameters=AzureOpenAIVectorizerParameters(\n",
    "                    resource_url=azure_openai_endpoint,\n",
    "                    deployment_name=azure_openai_embedding_deployment_name,\n",
    "                    model_name=azure_openai_embedding_deployment_name,\n",
    "                    api_key=azure_openai_key,\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Semantic search configuration for document retrieval\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"semantic-config\",\n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"title\"),\n",
    "            content_fields=[\n",
    "                SemanticField(field_name=\"content\"),\n",
    "                SemanticField(field_name=\"summary\"),\n",
    "            ],\n",
    "            keywords_fields=[\n",
    "                SemanticField(field_name=\"title\"),\n",
    "                SemanticField(field_name=\"file_name\"),\n",
    "                SemanticField(field_name=\"keywords\"),\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "    # Create the search index\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "        semantic_search=semantic_search,\n",
    "    )\n",
    "\n",
    "    result = index_client.create_or_update_index(index)\n",
    "    print(f\"Index '{result.name}' created or updated successfully.\")\n",
    "else:\n",
    "    print(\"Index creation is disabled. Set CREATE_INDEX = True to create the index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Intelligence PDF Upload Plugin\n",
    "\n",
    "Create upload_doc_plugin to process PDF files using Document Intelligence and upload to Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Intelligence endpoint: https://hyo-ai-foundry-pjt1-resource.cognitiveservices.azure.com/\n",
      "Document Intelligence client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Document Intelligence setup\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest, DocumentContentFormat\n",
    "import hashlib\n",
    "import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Document Intelligence environment variables\n",
    "document_intelligence_endpoint = get_env_var(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "document_intelligence_key = get_env_var(\"AZURE_DOCUMENT_INTELLIGENCE_API_KEY\")\n",
    "\n",
    "# Initialize Document Intelligence client\n",
    "doc_intelligence_client = DocumentIntelligenceClient(\n",
    "    endpoint=document_intelligence_endpoint,\n",
    "    credential=AzureKeyCredential(document_intelligence_key)\n",
    ")\n",
    "\n",
    "print(f\"Document Intelligence endpoint: {document_intelligence_endpoint}\")\n",
    "print(\"Document Intelligence client initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_doc_plugin(\n",
    "    file_path: str,\n",
    "    title: Optional[str] = None,\n",
    "    document_type: str = \"IR_REPORT\",\n",
    "    industry: Optional[str] = None,\n",
    "    company: Optional[str] = None,\n",
    "    report_year: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Upload and process PDF document using Document Intelligence and store in Azure AI Search.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the PDF file\n",
    "        title: title name (defaults to file_name)\n",
    "        document_type: Type of document (IR_REPORT, MARKET_RESEARCH, etc.)\n",
    "        industry: Industry category\n",
    "        company: Company name\n",
    "        report_year: Year of the report\n",
    "        author: Document author\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing upload status and document metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate document ID\n",
    "        file_name = os.path.basename(file_path)\n",
    "        doc_id = hashlib.md5(file_path.encode()).hexdigest()\n",
    "        \n",
    "        # Check if document already exists in the index\n",
    "        try:\n",
    "            existing_doc = search_client.get_document(key=doc_id)\n",
    "            return {\n",
    "                \"status\": \"already_exists\",\n",
    "                \"doc_id\": doc_id,\n",
    "                \"message\": f\"Document {file_name} already exists in the index\",\n",
    "                \"existing_doc\": existing_doc\n",
    "            }\n",
    "        except Exception:\n",
    "            # Document doesn't exist, proceed with upload\n",
    "            pass\n",
    "        \n",
    "        print(f\"Processing document: {file_name}\")\n",
    "        \n",
    "        # Step 1: Process PDF with Document Intelligence\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            poller = doc_intelligence_client.begin_analyze_document(\n",
    "                model_id=\"prebuilt-layout\",\n",
    "                body=file, \n",
    "                output_content_format=DocumentContentFormat.MARKDOWN\n",
    "            )\n",
    "        \n",
    "        result = poller.result()\n",
    "        \n",
    "        # Extract content and metadata\n",
    "        content = result.content if result.content else \"\"\n",
    "        # Extract metadata from document analysis\n",
    "        page_count = len(result.pages) if result.pages else 0\n",
    "        \n",
    "        # TODO Consider page level chunking (can be extended by length/token if needed)\n",
    "        # chunks = []\n",
    "        # for p in result.paragraphs or []:\n",
    "        #     page = p.bounding_regions[0].page_number if p.bounding_regions else None\n",
    "        #     chunks.append({\n",
    "        #         \"page\": page,\n",
    "        #         \"content\": p.content,\n",
    "        #         \"offset\": p.spans[0].offset,\n",
    "        #         \"length\": p.spans[0].length\n",
    "        #     })\n",
    "        # #\n",
    "        \n",
    "        # Step 2: Generate embeddings for content and summary\n",
    "        # Create summary from first 2000 characters of content\n",
    "        summary = content[:2000] + \"...\" if len(content) > 2000 else content\n",
    "        \n",
    "        # Generate content embedding\n",
    "        content_embedding_response = openai_client.embeddings.create(\n",
    "            input=content[:8000],  # Limit content for embedding\n",
    "            model=azure_openai_embedding_deployment_name\n",
    "        )\n",
    "        content_vector = content_embedding_response.data[0].embedding\n",
    "        \n",
    "        # Generate summary embedding\n",
    "        summary_embedding_response = openai_client.embeddings.create(\n",
    "            input=summary,\n",
    "            model=azure_openai_embedding_deployment_name\n",
    "        )\n",
    "        summary_vector = summary_embedding_response.data[0].embedding\n",
    "        \n",
    "        # Step 3: Extract keywords and topics from content\n",
    "        keywords = extract_keywords_from_content(content)\n",
    "        \n",
    "        # TODO Step 4: Generate topics, document tags, target audience using Azure OpenAI\n",
    "        target_audience=[\"investors\", \"analysts\", \"stakeholders\"]\n",
    "        topics=[\"financial_performance\", \"quarterly_results\", \"revenue\"]\n",
    "        document_tags=[\"earnings\", \"financial\"]\n",
    "        \n",
    "        # TODO Step 5: translate the content in Korean\n",
    "        content_ko = content  # Placeholder for translated content\n",
    "        \n",
    "        \n",
    "        # Step 4: Prepare document for upload\n",
    "        file_stats = os.stat(file_path)\n",
    "        current_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "        \n",
    "        document = {\n",
    "            \"docId\": doc_id,\n",
    "            \"content\": content,\n",
    "            \"content_vector\": content_vector,\n",
    "            \"summary\": summary,\n",
    "            \"summary_vector\": summary_vector,\n",
    "            \"title\": keywords,\n",
    "            \"file_name\": file_name,\n",
    "            \"file_hash\": file_name,\n",
    "            \"page_number\": \"1\",\n",
    "            \"paragraph_number\": \"1\",\n",
    "            \"chunk_number\": \"1\",\n",
    "            \"document_type\": document_type,\n",
    "            \"company\": company,\n",
    "            \"industry\": industry,\n",
    "            \"report_year\": report_year,\n",
    "            \"keywords\": keywords,\n",
    "            \"source\": file_path,\n",
    "            \"metadata\": json.dumps({\n",
    "                \"total_pages\": \"1\",\n",
    "                \"total_paragraphs\": \"1\",\n",
    "                \"total_chunks_in_paragraph\": \"1\",\n",
    "                \"file_size\": Path(file_path).stat().st_size,\n",
    "                \"processing_method\": \"document_intelligence\"\n",
    "            })\n",
    "        }\n",
    "\n",
    "        with open(\"document.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(document, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Step 5: Upload to Azure AI Search\n",
    "        search_client.upload_documents([document])\n",
    "        \n",
    "        print(f\"✅ Document {file_name} uploaded successfully!\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"doc_id\": doc_id,\n",
    "            \"doc_name\": file_name,\n",
    "            \"message\": f\"Document {file_name} processed and uploaded successfully\",\n",
    "            \"page_count\": page_count,\n",
    "            \"content_length\": len(content)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing document {file_path}: {str(e)}\")\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Error processing document: {str(e)}\"\n",
    "        }\n",
    "\n",
    "\n",
    "def extract_keywords_from_content(content: str, max_keywords: int = 20) -> str:\n",
    "    \"\"\"\n",
    "    TODO In production, you might want to use gpt models.\n",
    "    Extract keywords from document content using simple text processing.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Simple keyword extraction\n",
    "    # Remove common stop words and extract meaningful terms\n",
    "    stop_words = {\n",
    "        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
    "        'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',\n",
    "        'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those'\n",
    "    }\n",
    "    \n",
    "    # Extract words (letters only, minimum 3 characters)\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', content.lower())\n",
    "    \n",
    "    # Filter out stop words and count frequency\n",
    "    meaningful_words = [word for word in words if word not in stop_words]\n",
    "    word_counts = Counter(meaningful_words)\n",
    "    \n",
    "    # Get top keywords\n",
    "    top_keywords = [word for word, _ in word_counts.most_common(max_keywords)]\n",
    "    \n",
    "    return \", \".join(top_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Search Plugin for Document Retrieval\n",
    "\n",
    "Create ai_search_plugin to perform hybrid and semantic search on uploaded documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_search_plugin(\n",
    "    query: str,\n",
    "    search_type: str = \"hybrid\",  # \"hybrid\", \"semantic\", \"vector\", \"text\"\n",
    "    filters: Optional[str] = None,\n",
    "    top_k: int = 5,\n",
    "    include_content: bool = True,\n",
    "    document_type: Optional[str] = None,\n",
    "    industry: Optional[str] = None,\n",
    "    company: Optional[str] = None,\n",
    "    report_year: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Search documents in Azure AI Search using various search methods.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        search_type: Type of search (hybrid, semantic, vector, text)\n",
    "        filters: OData filter expression\n",
    "        top_k: Number of results to return\n",
    "        include_content: Whether to include full content in results\n",
    "        document_type: Filter by document type\n",
    "        industry: Filter by industry\n",
    "        company: Filter by company\n",
    "        report_year: Filter by report year\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing search results and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate query embedding for vector search\n",
    "        query_embedding_response = openai_client.embeddings.create(\n",
    "            input=query,\n",
    "            model=azure_openai_embedding_deployment_name\n",
    "        )\n",
    "        query_vector = query_embedding_response.data[0].embedding\n",
    "        \n",
    "        # Build filter expression\n",
    "        filter_parts = []\n",
    "        if filters:\n",
    "            filter_parts.append(filters)\n",
    "        if document_type:\n",
    "            filter_parts.append(f\"document_type eq '{document_type}'\")\n",
    "        if industry:\n",
    "            filter_parts.append(f\"industry eq '{industry}'\")\n",
    "        if company:\n",
    "            filter_parts.append(f\"company eq '{company}'\")\n",
    "        if report_year:\n",
    "            filter_parts.append(f\"report_year eq '{report_year}'\")\n",
    "        \n",
    "        filter_expression = \" and \".join(filter_parts) if filter_parts else None\n",
    "        \n",
    "        # Configure search based on search type\n",
    "        search_results = None\n",
    "        \n",
    "        if search_type == \"hybrid\":\n",
    "            # Hybrid search: combines text and vector search\n",
    "            vector_queries = [\n",
    "                VectorizedQuery(\n",
    "                    vector=query_vector,\n",
    "                    k_nearest_neighbors=top_k * 2,  # Get more for reranking\n",
    "                    fields=\"content_vector\"\n",
    "                ),\n",
    "                VectorizedQuery(\n",
    "                    vector=query_vector,\n",
    "                    k_nearest_neighbors=top_k,\n",
    "                    fields=\"summary_vector\"\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            search_results = search_client.search(\n",
    "                search_text=query,\n",
    "                vector_queries=vector_queries,\n",
    "                filter=filter_expression,\n",
    "                query_type=QueryType.SEMANTIC,\n",
    "                semantic_configuration_name=\"semantic-config\",\n",
    "                query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "                query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "                top=top_k,\n",
    "                select=\"docId,title,file_name,document_type,content,summary,industry,company,report_year,page_number,upload_date,keywords\" if include_content else \"docId,title,file_name,document_type,summary,industry,company,report_year,page_number,upload_date\"\n",
    "            )\n",
    "            \n",
    "        elif search_type == \"semantic\":\n",
    "            # Semantic search only\n",
    "            search_results = search_client.search(\n",
    "                search_text=query,\n",
    "                filter=filter_expression,\n",
    "                query_type=QueryType.SEMANTIC,\n",
    "                semantic_configuration_name=\"semantic-config\",\n",
    "                query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "                query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "                top=top_k,\n",
    "                select=\"docId,title,file_name,document_type,content,summary,industry,company,report_year,page_number,upload_date,keywords\" if include_content else \"docId,title,file_name,document_type,summary,industry,company,report_year,page_number,upload_date\"\n",
    "            )\n",
    "            \n",
    "        elif search_type == \"vector\":\n",
    "            # Vector search only\n",
    "            vector_queries = [\n",
    "                VectorizedQuery(\n",
    "                    vector=query_vector,\n",
    "                    k_nearest_neighbors=top_k,\n",
    "                    fields=\"content_vector\"\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            search_results = search_client.search(\n",
    "                search_text=None,\n",
    "                vector_queries=vector_queries,\n",
    "                filter=filter_expression,\n",
    "                top=top_k,\n",
    "                select=\"docId,title,file_name,document_type,content,summary,industry,company,report_year,page_number,upload_date,keywords\" if include_content else \"docId,title,file_name,document_type,summary,industry,company,report_year,pageCount,upload_date\"\n",
    "            )\n",
    "            \n",
    "        elif search_type == \"text\":\n",
    "            # Text search only\n",
    "            search_results = search_client.search(\n",
    "                search_text=query,\n",
    "                filter=filter_expression,\n",
    "                top=top_k,\n",
    "                select=\"docId,title,file_name,document_type,content,summary,industry,company,report_year,page_number,upload_date,keywords\" if include_content else \"docId,title,file_name,document_type,summary,industry,company,report_year,pageCount,upload_date\"\n",
    "            )\n",
    "        \n",
    "        # Process results\n",
    "        documents = []\n",
    "        for result in search_results:\n",
    "            doc = dict(result)\n",
    "            \n",
    "            # Add search metadata\n",
    "            if hasattr(result, '@search.score'):\n",
    "                doc['search_score'] = result['@search.score']\n",
    "            if hasattr(result, '@search.reranker_score'):\n",
    "                doc['reranker_score'] = result['@search.reranker_score']\n",
    "            if hasattr(result, '@search.captions'):\n",
    "                doc['captions'] = [caption.text for caption in result['@search.captions']]\n",
    "            \n",
    "            # Truncate content if needed for response size\n",
    "            if include_content and 'content' in doc and len(doc['content']) > 2000:\n",
    "                doc['content_preview'] = doc['content'][:2000] + \"...\"\n",
    "                if not include_content:\n",
    "                    del doc['content']\n",
    "            \n",
    "            documents.append(doc)\n",
    "        \n",
    "        # Get search answers if available\n",
    "        answers = []\n",
    "        if hasattr(search_results, 'get_answers') and search_results.get_answers():\n",
    "            answers = [{\"text\": answer.text, \"score\": answer.score} for answer in search_results.get_answers()]\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"query\": query,\n",
    "            \"search_type\": search_type,\n",
    "            \"filter\": filter_expression,\n",
    "            \"total_results\": len(documents),\n",
    "            \"documents\": documents,\n",
    "            \"answers\": answers,\n",
    "            \"message\": f\"Found {len(documents)} documents for query: {query}\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error searching documents: {str(e)}\")\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Error searching documents: {str(e)}\"\n",
    "        }\n",
    "\n",
    "\n",
    "def get_document_by_id(doc_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieve a specific document by its ID.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: Document ID\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing document data or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        document = search_client.get_document(key=doc_id)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"document\": dict(document)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Document not found: {str(e)}\"\n",
    "        }\n",
    "\n",
    "\n",
    "def list_documents(\n",
    "    document_type: Optional[str] = None,\n",
    "    industry: Optional[str] = None,\n",
    "    company: Optional[str] = None,\n",
    "    top: int = 20\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    List documents with optional filtering.\n",
    "    \n",
    "    Args:\n",
    "        document_type: Filter by document type\n",
    "        industry: Filter by industry\n",
    "        company: Filter by company\n",
    "        top: Number of documents to return\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing list of documents\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build filter\n",
    "        filter_parts = []\n",
    "        if industry:\n",
    "            filter_parts.append(f\"industry eq '{industry}'\")\n",
    "        if document_type:\n",
    "            filter_parts.append(f\"document_type eq '{document_type}'\")\n",
    "        if company:\n",
    "            filter_parts.append(f\"company eq '{company}'\")\n",
    "        \n",
    "        filter_expression = \" and \".join(filter_parts) if filter_parts else None\n",
    "        \n",
    "        results = search_client.search(\n",
    "            search_text=\"*\",\n",
    "            filter=filter_expression,\n",
    "            top=top,\n",
    "            select=\"docId,title,file_name,industry,company,report_year,page_number,upload_date,summary\",\n",
    "            order_by=[\"upload_date desc\"]\n",
    "        )\n",
    "        \n",
    "        documents = [dict(result) for result in results]\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"total_results\": len(documents),\n",
    "            \"documents\": documents\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Error listing documents: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Examples and Testing\n",
    "\n",
    "Examples of how to use the upload_doc_plugin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document: IR_251Q_CASA.pdf\n",
      "✅ Document IR_251Q_CASA.pdf uploaded successfully!\n",
      "Upload Result:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"doc_id\": \"f27c9d12fe31888d6fe283ec21ac696c\",\n",
      "  \"doc_name\": \"IR_251Q_CASA.pdf\",\n",
      "  \"message\": \"Document IR_251Q_CASA.pdf processed and uploaded successfully\",\n",
      "  \"page_count\": 78,\n",
      "  \"content_length\": 156356\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Upload a PDF document\n",
    "UPLOAD_TEST_DOCUMENT = False\n",
    "\n",
    "if UPLOAD_TEST_DOCUMENT:\n",
    "    # Example PDF file path (replace with your actual PDF file path)\n",
    "    pdf_file_path = \"IR_251Q_CASA.pdf\"\n",
    "    \n",
    "    # Upload document with metadata\n",
    "    upload_result = upload_doc_plugin(\n",
    "        file_path=pdf_file_path,\n",
    "        title=\"CASA Report\",\n",
    "        document_type=\"IR_REPORT\",\n",
    "        industry=\"Technology\",\n",
    "        company=\"ACredit Agricole Group\",\n",
    "        report_year=\"2025\"\n",
    "    )\n",
    "    \n",
    "    print(\"Upload Result:\")\n",
    "    print(json.dumps(upload_result, indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(\"Document upload test is disabled. Set UPLOAD_TEST_DOCUMENT = True to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search documents using different methods\n",
    "\n",
    "Examples of how to use the ai_search_plugin functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Document Search ===\n",
      "\n",
      "\n",
      "4. List All Documents:\n",
      "Status: success\n",
      "Total documents: 1\n",
      "  1. N/A - None\n"
     ]
    }
   ],
   "source": [
    "SEARCH_TEST = True\n",
    "\n",
    "if SEARCH_TEST:\n",
    "    print(\"=== Testing Document Search ===\\n\")\n",
    "    \n",
    "    # Test 1: Hybrid search\n",
    "    # print(\"1. Hybrid Search:\")\n",
    "    # hybrid_results = ai_search_plugin(\n",
    "    #     query=\"CASA Report 2025\",\n",
    "    #     search_type=\"hybrid\",\n",
    "    #     top_k=3,\n",
    "    #     include_content=True\n",
    "    # )\n",
    "    # print(f\"Status: {hybrid_results['status']}\")\n",
    "    # print(f\"Found {hybrid_results.get('total_results', 0)} documents\")\n",
    "    # if hybrid_results['status'] == 'success':\n",
    "    #     for i, doc in enumerate(hybrid_results['documents'][:2], 1):\n",
    "    #         print(f\"  {i}. {doc.get('docName', 'N/A')} ({doc.get('company', 'N/A')})\")\n",
    "    #         print(\" ==============📄content[:100]===============\")\n",
    "    #         print(f\"{doc.get('content', 'N/A')[:100]}\")\n",
    "    # print()\n",
    "    \n",
    "    # Test 2: Semantic search with filters\n",
    "    # print(\"2. Semantic Search with Filters:\")\n",
    "    # semantic_results = ai_search_plugin(\n",
    "    #     query=\"revenue growth and market analysis\",\n",
    "    #     search_type=\"semantic\",\n",
    "    #     document_type=\"IR_REPORT\",\n",
    "    #     company=\"ACredit Agricole Group\",\n",
    "    #     top_k=3,\n",
    "    #     include_content=True\n",
    "    # )\n",
    "    # print(f\"Status: {semantic_results['status']}\")\n",
    "    # print(f\"Found {semantic_results.get('total_results', 0)} documents\")\n",
    "    # if semantic_results['status'] == 'success':\n",
    "    #     for i, doc in enumerate(semantic_results['documents'][:2], 1):\n",
    "    #         print(f\"  {i}. {doc.get('docName', 'N/A')} - {doc.get('documentType', 'N/A')}\")\n",
    "    #         print(\" ==============📄content[:100]===============\")\n",
    "    #         print(f\"{doc.get('content', 'N/A')[:100]}\")\n",
    "    # print()\n",
    "    \n",
    "    # Test 3: Vector search\n",
    "    # print(\"3. Vector Search:\")\n",
    "    # vector_results = ai_search_plugin(\n",
    "    #     query=\"financial metrics and KPI analysis\",\n",
    "    #     search_type=\"vector\",\n",
    "    #     top_k=3,\n",
    "    #     include_content=True\n",
    "    # )\n",
    "    # print(f\"Status: {vector_results['status']}\")\n",
    "    # print(f\"Found {vector_results.get('total_results', 0)} documents\")\n",
    "    # if vector_results['status'] == 'success':\n",
    "    #     for i, doc in enumerate(vector_results['documents'][:2], 1):\n",
    "    #         print(f\"  {i}. {doc.get('docName', 'N/A')} (Score: {doc.get('search_score', 'N/A')})\")\n",
    "    #         print(\" ==============📄content[:100]===============\")\n",
    "    #         print(f\"{doc.get('content', 'N/A')[:100]}\")\n",
    "    print()\n",
    "    \n",
    "    # Test 4: List all documents\n",
    "    print(\"4. List All Documents:\")\n",
    "    list_results = list_documents(top=5)\n",
    "    print(f\"Status: {list_results['status']}\")\n",
    "    print(f\"Total documents: {list_results.get('total_results', 0)}\")\n",
    "    if list_results['status'] == 'success':\n",
    "        for i, doc in enumerate(list_results['documents'][:3], 1):\n",
    "            print(f\"  {i}. {doc.get('docName', 'N/A')} - {doc.get('upload_date', 'N/A')}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Search test is disabled. Set SEARCH_TEST = True to test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc-inquiry-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
